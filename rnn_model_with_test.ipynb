{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iLph4wafb2R"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import csv\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "\n",
    "#load the data\n",
    "file = open(\"drugsComTrain_raw.tsv\",\"r\").readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KYKm-Cofb2p",
    "outputId": "bb49bac4-1042-47c9-c5b1-bc09f983a00c"
   },
   "outputs": [],
   "source": [
    "for i in range(len(file)):\n",
    "    file[i] = file[i].split('\\t')\n",
    "\n",
    "df = pd.DataFrame(data = file[1:], columns = file[0])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "WQZRDeS7fb3Y",
    "outputId": "6388589b-3b4d-4efa-9702-d1dc51ba8071"
   },
   "outputs": [],
   "source": [
    "reviews = list(df['review'])\n",
    "numTweets = len(reviews)\n",
    "print(numTweets)\n",
    "#print(type(reviews[0]),type(reviews[numTweets-1]))\n",
    "\n",
    "#counting the total number of words\n",
    "numWords = []\n",
    "count = 0\n",
    "for i in range(numTweets):\n",
    "    if(reviews[i]):\n",
    "        line = reviews[i].split()\n",
    "        print(line)\n",
    "        counter = int(len(line))\n",
    "        numWords.append(counter)\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))\n",
    "\n",
    "maxSeqLength = max(numWords) #longest review in the existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USBmGfZ5136L",
    "outputId": "7d48aec9-0403-438b-d1c3-d95b21f3eb72"
   },
   "outputs": [],
   "source": [
    "#print(numWords)\n",
    "maxSeqLength = max(numWords)\n",
    "print(numWords.index(maxSeqLength))\n",
    "print(maxSeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGi19hhf136c"
   },
   "outputs": [],
   "source": [
    "#uploaded = files.upload()\n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0nskJ0c136s"
   },
   "outputs": [],
   "source": [
    "#load the numbered word vectors\n",
    "#uploaded = files.upload()\n",
    "wordsList = np.load('wordsList.npy')\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySYndZuxgPPZ"
   },
   "outputs": [],
   "source": [
    "#load the numbered word vectors\n",
    "#uploaded = files.upload()\n",
    "wordsList = np.load('wordsList.npy')\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]\n",
    "#uploaded = files.upload()\n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8wvOlki137R",
    "outputId": "4c0b4c75-2733-46a5-c513-a0418d3dec65"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 500)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([125, 1500, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLZUS0IkgEJb"
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    #phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    #phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    string = string.lower().replace(\"\\n\", \"\")\n",
    "    string = decontracted(string)\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3htKRnGjryo",
    "outputId": "d9e330f0-de0a-4564-8f15-80cd05c72a07",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#defining an ids matrix for the word vectors from the list\n",
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "reviewCounter = 0\n",
    "for i in range(10):\n",
    "    index = 0\n",
    "    if reviews[i]:\n",
    "        cleanedLine = cleanSentences(reviews[i])\n",
    "        print(cleanedLine)\n",
    "        line = cleanedLine.split(' ')\n",
    "        for word in line:\n",
    "            try:\n",
    "                ids[reviewCounter][index] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[reviewCounter][index] = 399999 #Vector for unkown words\n",
    "            index = index + 1\n",
    "        if index >= maxSeqLength:\n",
    "            break\n",
    "        reviewCounter = reviewCounter + 1\n",
    "\n",
    "np.save('idsMatrix_trained',ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHNrpmwR138T",
    "outputId": "e7b868b2-e742-4ec3-aeec-1279bdf49450"
   },
   "outputs": [],
   "source": [
    "idMat = np.load('idsMatrix_model.npy')\n",
    "len(idMat) #179281 #44820(1/4th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LtimYsp138e",
    "outputId": "b1092213-9bd9-486d-b509-8bdca6d611e9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(idMat) #1760.34436180006\n",
    "print(idMat.shape)\n",
    "import collections\n",
    "list_of_outlier_words = {}\n",
    "for i in range(numFiles):\n",
    "    no_of_unwanted_words = dict(collections.Counter(idMat[i]))\n",
    "    #print(no_of_unwanted_words)\n",
    "    list_of_outlier_words[i] = no_of_unwanted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNYIgBBO138v",
    "outputId": "13ea85c1-029a-4a8e-e6a6-266b6b6de547",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list_of_outlier_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8t9KojRksk7"
   },
   "source": [
    "**Splitting Data Batch-wise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KQQdgRyk0Nm"
   },
   "outputs": [],
   "source": [
    "from random import randint   #1/4th of the training set\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,82462) #1,20616\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(96805,179274) #24201, 44819\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(82462, 96805) #20616, 24201 \n",
    "        if (num <= 89633):          #22408\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAYjq0wGkUOd"
   },
   "source": [
    "**RNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzCxzXcMkW8_"
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hhHzIEhqmzm"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tvp1ciLsqtQu"
   },
   "outputs": [],
   "source": [
    "numDimensions = 300\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HwgeMjaqvvJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "lstmCell = tf.nn.rnn_cell.LSTMCell(lstmUnits)\n",
    "lstmCell = tf.nn.rnn_cell.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpTNMZuSrGLp"
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwdcJ5-prJu6"
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Auq07kcnrMCv"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXHfA-u9rOKn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4npxoEmrXos"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqTM0LBLrW9d",
    "outputId": "e19134d5-e593-490a-9722-718e5c4ffeeb"
   },
   "outputs": [],
   "source": [
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "  #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0): #50\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        print(\"Summary added %d\"%(i))\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 1000 == 0 and i != 0): #10000\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GT8rku3q13_3",
    "outputId": "845c469f-e444-4eca-fba4-a4799a89c8f7"
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtJFrvhP14AK"
   },
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKsG_WVA14AM",
    "outputId": "c94da6e2-4277-45a2-e708-e60cdbc23c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for this batch:', 70.83333134651184)\n",
      "('Accuracy for this batch:', 45.83333432674408)\n",
      "('Accuracy for this batch:', 37.5)\n",
      "('Accuracy for this batch:', 45.83333432674408)\n",
      "('Accuracy for this batch:', 50.0)\n",
      "('Accuracy for this batch:', 41.66666567325592)\n",
      "('Accuracy for this batch:', 58.33333134651184)\n",
      "('Accuracy for this batch:', 41.66666567325592)\n",
      "('Accuracy for this batch:', 50.0)\n",
      "('Accuracy for this batch:', 58.33333134651184)\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyQYCw1S14AR"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33O7uXmf14AS"
   },
   "outputs": [],
   "source": [
    "test_file = open(\"drugsComTest_raw.tsv\",\"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CAvdlXh214AZ",
    "outputId": "04cc94d5-fdeb-4eee-d5a8-3a51aed31c04"
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_file)):\n",
    "    test_file[i] = test_file[i].split('\\t')\n",
    "\n",
    "test_df = pd.DataFrame(data = test_file[1:], columns = test_file[0])\n",
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0k0eO29I14Ar",
    "outputId": "98ff8c00-6568-4c1b-d89e-2fdff3b42303"
   },
   "outputs": [],
   "source": [
    "test_reviews = list(test_df['review'])\n",
    "test_numTweets = len(test_reviews)\n",
    "print(test_numTweets)\n",
    "print(type(test_reviews[0]),type(test_reviews[test_numTweets-1]))\n",
    "\n",
    "#counting the total number of words\n",
    "test_numWords = []\n",
    "t_count = 0\n",
    "for i in range(test_numTweets):\n",
    "    if(test_reviews[i]):\n",
    "        line = test_reviews[i].split()\n",
    "        print(line)\n",
    "        counter = int(len(line))\n",
    "        test_numWords.append(counter)\n",
    "\n",
    "test_numFiles = len(test_numWords)\n",
    "print('The total number of files is', test_numFiles)\n",
    "print('The total number of words in the files is', sum(test_numWords))\n",
    "print('The average number of words in the files is', sum(test_numWords)/len(test_numWords))\n",
    "\n",
    "test_maxSeqLength = max(test_numWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lS9dZEdT14BF",
    "outputId": "739c6c9b-3dae-4a32-ee6f-1841565b8e56"
   },
   "outputs": [],
   "source": [
    "test_maxSeqLength = max(test_numWords)\n",
    "print(test_numWords.index(test_maxSeqLength))\n",
    "print(test_maxSeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cupydwu614BL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"\\n\", \"\")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRY0DhYm14Bc",
    "outputId": "a3e554d7-bb11-4099-cf64-b8050a7c9da8"
   },
   "outputs": [],
   "source": [
    "rand_index = randint(0, len(test_reviews)-1)\n",
    "print(rand_index)\n",
    "inputText = test_reviews[rand_index]\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L18D56oK14Bg",
    "outputId": "9e4c0fa2-8236-4a85-bdbc-b9d9ccba87b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(inputText)\n",
    "print(inputMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vvew0qMm14Bn",
    "outputId": "46a03f72-aa1a-465e-b990-cc5dd0e839b9"
   },
   "outputs": [],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "# predictedSentiment[0] represents output score for positive sentiment\n",
    "# predictedSentiment[1] represents output score for negative sentiment\n",
    "\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sample-rnn-model-with-test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
